# [머신러닝] 머신러닝에 대한 정리







## 1. 머신러닝이란?

+ 머신러닝이란 명시적으로 프로그래밍된 코드를 그대로 실행하는 것이 아니라, 경험을 바탕으로 해서 주어진 작업을 하도록 머신, 컴퓨터를 가르치는 것을 말합니다. 사람이 상황을 기억하고 이를 바탕으로 예측하는 것처럼, 컴퓨터도 방대한 데이터셋을 저장하고, 그 속에서 유의미한 규칙 또는 모델을 찾고, 이 규칙 또는 모델을 적용해 미래의 데이터에 대해 예측합니다. 머신러닝에서는 이 모델을 정의하기 위해 라벨링이 중요합니다. 라벨은 데이터로부터 예측하고자 하는 특징을 지칭합니다. 강아지와 고양이 사진을 잔뜩 주고 ‘강아지’, ‘고양이’라는 이름을 붙이는 것이 라벨링입니다. 
+ 머신러닝은 지도학습, 비지도학습, 강화학습의 세 가지 분류가 있습니다. 먼저, 지도학습이란 학습할 데이터와 함께 라벨이 함께 주어지는 학습을 말합니다. 이 지도 학습에는 다시 대표적으로 회귀 모델과 분류 모델이 있습니다. 회귀 모델은 주택 가격과 같이 연속된 값을 예측하는 모델입니다. 데이터들과 최대한 가까운 선을 그려 미래의 데이터를 예측합니다. 분류 모델은 불연속적인 값을 예측하는 모델로, 데이터를 가장 잘 나누는 선을 찾는 모델입니다. 결정나무 기법 등이 있습니다. 
+ 다음으로, 비지도 학습은 학습할 데이터는 주어지나 라벨이 없는 경우의 학습입니다. 예를 들어 라벨이 붙지 않은 사진들이 주어지면 사진들 간의 유사성을 기준으로 사진을 그룹화합니다. 뉴스기사를 자동 묶음 하는 것도 이런 비지도학습이 적용된다고 합니다. 대표적으로 클러스터링이 있습니다. 
+ 강화학습은 배우지 않아 잘 모르는 부분이지만, 시행착오를 통해 실수와 보상이 반복되면서 학습하는 방법이라고 알고 있습니다.
+ 머신러닝에서 모델을 선정하고 학습률을 향상시키며 모델의 성능을 평가하기 위해서는 데이터를 훈련용/검증용/테스트용으로 나누는 것이 중요합니다. 이렇게 데이터를 나누는 또 하나의 이유는 과적합을 방지하기 위해서이기도 합니다. 과적합이란 모델이 훈련용 데이터에만 최적화되어 일반성을 가지지 못하게 되는 현상을 의미합니다. 이러한 사태를 방지하기 위해 충분한 데이터셋을 통한 검증과 테스트가 필요합니다.







## 2. 선형 회귀 모델

+ 선형 회귀모델에서는 데이터를 가장 잘 나타내는 직선의 방정식을 찾습니다. 초기 랜덤한 기울기와 y절편을 갖고 있다가, 데이터셋에서 임의의 데이터를 선택해 직선의 기울기와 y절편의 값을 조금씩 바꿔 데이터에 직선이 가까워지게 하는 것을 계속 반복하는 것입니다. 
+ 직선이 각 데이터에 얼마나 가까워지게 할 것인가를 구하기 위해 비용함수를 이용하여 계산하게 됩니다. 각 데이터로부터 직선까지의 수직 거리의 제곱의 합계를 Square Error라고 하며, 이것의 평균이 MSE를 비용함수라고 합니다. 이 비용함수가 최소가 되도록 하는 기울기 세타를 구해 직선의 방정식을 구할 수 있습니다. 여기에 경사하강법이라는 알고리즘이 사용됩니다. 학습률 알파를 설정하고, 초기에 랜덤으로 지정한 직선의 기울기 세타에서 학습률을 계속 빼거나 더하며 비용 함수가 최소가 될 때까지, 즉 세타의 절대값이 가장 작아질때까지 반복합니다. 이 학습률이 작다면 결과의 정확성이 올라가지만 속도가 느리고, 학습률이 크다면 속도가 빨라지지만 원하는 결과를 제대로 얻지 못할 수 있습니다. 또한 세타 수렴하지 못하고 진동할 가능성도 있습니다. 따라서 학습률을 잘 조정하는 것도 좋은 모델의 큰 관건이 됩니다. 
+ 이렇게 구한 선형회귀모델을 이용해 보스턴 집값을 예측하는 실습을 해봤었습니다. seaborn이라는 라이브러리의 히트맵을 이용해 데이터셋에서 집값과 상관관계가 높은 특징을 찾아내고, 해당 특징과 집값을 x, y축으로 설정하여 사이킷런의 리니어리그레션 모델을 이용해 구했던 기억이 납니다.







## 3. 분류 모델

+ 분류모델에서는 두 데이터를 가장 잘 나누기위한 선을 찾고 카테고리를 예측합니다. 스팸 메일 여부를 결정하거나 종양의 악성 여부, 날씨를 예측하는 등의 문제에 사용할 수 있습니다. 분류 모델에서 데이터를 잘 나누는 선, classifier를 찾기 위해서는 다음과 같은 과정들을 수행합니다. 먼저, classifier를 임의로 정합니다. 그 다음 학습 데이터셋에서 임의의 데이터를 선택하여 classifier가 이 데이터셋을 잘 분류하였으면 놔두고, 잘못 분류하였으면 이 classifier를 데이터에 가까워지게 조금 이동시킵니다. 이 과정을 반복하여 분류 모델을 완성합니다. 
+ 분류 모델에서의 비용함수는 선형 회귀 모델처럼 수직 거리의 합계와 평균으로 계산하지 않습니다. 계산의 방법이 너무 복잡하기 때문입니다. 따라서, 에러에 해당하는 데이터에 대해 Score를 계산하여 비용을 계산합니다. 직선의 방정식이 있을 때, 에러 데이터에 대한 x, y값을 넣어 계산된 값을 Score로 보고, 이 Score들의 합을 통해 비용을 추출합니다.
+ 결정나무 기법은 분류모델을 만드는 알고리즘으로, 사람의 추론 방식과 닮아있는 형태의 기법입니다. 계속해서 질문을 해나가며 대답에 따라 가지를 뻗고 주어진 데이터들을 분류합니다. 이 결정나무 기법을 통해서 머신러닝의 가장 기초가 되는 실습인 아이리스 꽃 분류하기를 해봤었습니다. 사이킷런의 decisiontreeclassifier를 임포트해서 진행했습니다. 결정나무의 최대 깊이와 데이터, 그리고 기준이 될 특징들을 지정하면 바로 끝나게 됩니다. 깊이가 깊을수록 분류는 세세해지지만, 너무 깊으면 확인도 힘들고 과적합이 발생할 수 있습니다. 주어진 데이터셋을 train_test_split을 이용해 훈련용과 테스트용으로 만든 뒤, 훈련용 데이터셋으로 모델을 만들고 predict 함수와 테스트 데이터셋을 이용해 모델의 정확도를 평가할 수 있습니다. 또, 어떤 특징이 분류의 중요한 기준이 되었는지도 확인이 가능합니다.







## 4. 커널 기법

+ 커널기법은 주어지는 입력 변수가 여러 개여서 다중선형회귀로 문제를 해결해야할 경우 사용할 수 있는 기법입니다. 주어진 데이터셋을 직선으로 나눌 수 없을 때, 이 데이터셋을 둘로 나누는 곡선을 찾기 위해 고안되었습니다. 
+ 우선, 보통의 x, y축 그래프에서 입력 변수, z축을 추가하여 데이터셋을 3차원으로 확장합니다. 그 다음 이 확장된 데이터 셋에서 데이터를 둘로 잘 나누는 평면을 찾습니다. 데이터셋들이 나타내는 포물면과 이 평면이 교차하는 부분을 2차원으로 투영하면 원 형태의 classifier를 얻을 수 있습니다. 이러한 기법을 커널 기법이라고 합니다.







## 5. 검증

+ 모델을 만드는 데 있어 훈련용/테스트용 한 세트만 만들면 편향현상과 같은 이상 결과를 얻을 수 있습니다. 따라서 훈련용 데이터 셋을 다시 훈련용과 검증용으로 나눠주는 작업이 필요합니다. 쉽게 말해 테스트라는 본고사를 치르기 전에 검증용으로 모의고사를 치른다고 생각할 수 있습니다. 
+ 이때 가장 많이 사용되는 검증 알고리즘이 K-교차 검증입니다. k개로 나눈 훈련용 데이터 셋 중 하나를 검증용 데이터 셋으로 설정해 테스트를 진행하고, 이 과정을 모든 나눠진 데이터 셋에 수행하여 총 k번 테스트를 합니다. 그리고 각 테스트들의 평가를 평균 내어 훈련용 데이터 셋의 검증을 완료합니다. 정리하자면, 집합을 체계적으로 바꿔가면서 모든 데이터에 대해 모형의 성과를 측정하는 검증 방식입니다.







## 6. 앙상블 학습

+ 여러 개의 Classifier를 생성하고 그 예측을 결합하여 정확한 최종 에측을 기대하는 기법입니다. 정형 데이터를 대상으로 할 때 매우 뛰어난 성능을 보여줍니다. 
+ 대표적으로 배깅과 부스팅이 있는데, 먼저 배깅은 데이터의 중복을 허용하여 샘플링을 한 뒤 각각의 데이터에 같은 알고리즘을 적용해서 결과를 투표로 결정하는 방식입니다. 이때 각각의 분류기에 데이터를 샘플링해서 추출하는 방식을 부트스트래핑 분할 방식이라고 합니다. 대표적인 알고리즘으로는 각각의 샘플에 결정나무 기법을 적용하는 랜덤 포레스트가 있습니다. 
+ 다음으로, 부스팅은 여러 개의 약한 Classifier가 순차적으로 학습을 하면서, 앞에서 학습한 Classifier가 예측이 틀린 데이터에 대해 다음 Classifier가 가중치를 인가해서 학습을 이어 진행하는 방식입니다. 잘못 분류한 데이터의 가중치는 높이고, 맞게 분류한 데이터의 가중치는 낮춥니다. 배깅이 병렬적 방식이라면 부스팅은 직렬적 방식이라는 차이가 있습니다.







## 7. 클러스터링

+ 클러스터링은 비지도학습의 알고리즘으로, 군집화라는 의미를 가집니다. 즉, 데이터들을 유사성을 기준으로 군집시키는 것인데, k-means 알고리즘이 가장 대표적입니다. 
+ k-means 알고리즘에서는 먼저 특징을 축으로 하는 좌표평면 상의 데이터 셋 위에 임의의 중심점 2개를 설정합니다. 그리고 각 데이터 별로 가장 가까운 중심점에 클러스터 할당합니다. 각 클러스터에 할당된 데이터들의 평균 위치로 중심점을 이동시킵니다. 이 과정을 반복하다가 중심점이 더 이상 이동하지 않으면 알고리즘이 종료됩니다. 이를 통해 특징을 기반으로 한 군집화가 가능합니다.
